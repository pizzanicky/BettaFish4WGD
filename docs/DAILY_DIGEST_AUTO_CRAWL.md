# Daily Digest - 一键爬取+生成功能说明

## ✨ 新功能：自动爬取 + 生成摘要

现在无需手动在容器内执行爬取命令，直接在Streamlit页面上一键完成！

---

## 🎯 功能特性

### **1. 一键式工作流**
- ✅ 输入关键词
- ✅ 自动爬取Reddit最新数据
- ✅ 实时显示爬取进度
- ✅ 自动生成情绪摘要
- ✅ 展示精美的分析卡片

### **2. 实时进度显示**
使用 `st.status()` 组件显示：
- 📡 步骤 1/2: 爬取Reddit数据...
- 📊 步骤 2/2: 生成情绪摘要...
- ✅ 处理完成！

### **3. 灵活配置**
- **自动爬取开关**：可选择是否自动爬取
- **爬取数量控制**：50-200条帖子可调
- **时间窗口**：1-72小时可选

---

## 📋 使用步骤

### **方式1：一键爬取+生成（推荐）**

1. **打开页面**
   ```
   http://localhost:8501
   ```

2. **配置参数**
   - 输入关键词（如：`TSLA`, `NVDA`）
   - 选择时间窗口（默认24小时）
   - ✅ 勾选"自动爬取数据"
   - 设置最大爬取帖子数（默认100条）

3. **点击按钮**
   - 点击"🚀 生成 Digest"

4. **查看进度**
   ```
   🔄 正在处理...
   📡 步骤 1/2: 爬取Reddit数据...
   ✅ 成功爬取 100 条帖子
   📊 步骤 2/2: 生成情绪摘要...
   ✅ 处理完成！
   ```

5. **查看结果**
   - 精美的情绪卡片
   - 详细的摘要分析
   - 热门讨论列表

### **方式2：仅生成摘要（使用已有数据）**

1. **取消勾选** "自动爬取数据"
2. **点击** "🚀 生成 Digest"
3. 系统会使用数据库中已有的数据生成摘要

---

## 🔧 技术实现

### **核心函数**

#### **1. `run_crawl()` - 爬取函数**
```python
def run_crawl(keyword: str, max_count: int = 100):
    """
    同步执行爬取
    返回: (success: bool, message: str, post_count: int)
    """
```

#### **2. `run_crawl_and_digest()` - 一键函数**
```python
def run_crawl_and_digest(keyword: str, hours: int = 24, max_count: int = 100):
    """
    一键执行：爬取 + 生成摘要
    返回: {
        "crawl_success": bool,
        "crawl_message": str,
        "post_count": int,
        "digest_result": dict
    }
    """
```

### **工作流程**

```
用户输入关键词
    ↓
点击"生成 Digest"
    ↓
[步骤1] 调用 run_crawl()
    ├─ 初始化 RedditCrawler
    ├─ 设置关键词
    ├─ 爬取100条帖子
    └─ 保存到PostgreSQL
    ↓
[步骤2] 调用 run_digest_generation()
    ├─ 从数据库读取帖子
    ├─ 调用 Google Gemini API
    ├─ 生成情绪分析
    └─ 返回结果
    ↓
显示结果
    ├─ 情绪卡片
    ├─ 摘要内容
    └─ 热门讨论
```

---

## 📊 界面说明

### **侧边栏配置**

```
┌─────────────────────────┐
│  Configuration          │
├─────────────────────────┤
│  Keyword: [TSLA      ]  │
│  Time Window: [24h]     │
├─────────────────────────┤
│  🕷️ 爬取选项            │
│  ☑ 自动爬取数据         │
│  最大爬取帖子数: 100    │
├─────────────────────────┤
│  [🚀 生成 Digest]       │
└─────────────────────────┘
```

### **主界面显示**

```
┌──────────────────────────────────────┐
│  📰 Daily Sentiment Digest           │
│  一键爬取Reddit数据并生成情绪摘要分析│
├──────────────────────────────────────┤
│  🔄 正在处理...                      │
│  ✅ 成功爬取 100 条帖子              │
│  ✅ 处理完成！                       │
├──────────────────────────────────────┤
│  [情绪卡片]                          │
│  ┌────────────┐                      │
│  │   TSLA     │                      │
│  │   看涨     │                      │
│  │  财报超预期 │                      │
│  └────────────┘                      │
├──────────────────────────────────────┤
│  📝 Summary    │  🔥 热门讨论        │
│  [摘要内容]    │  [帖子列表]         │
└──────────────────────────────────────┘
```

---

## ⚙️ 配置选项

### **爬取参数**

| 参数 | 默认值 | 范围 | 说明 |
|------|--------|------|------|
| 关键词 | - | 任意 | 搜索的股票代码或关键词 |
| 时间窗口 | 24小时 | 1-72小时 | 分析的时间范围 |
| 自动爬取 | ✅ 开启 | 开/关 | 是否自动爬取数据 |
| 最大帖子数 | 100条 | 50-200条 | 每次爬取的最大数量 |

### **爬取配置（后端）**

在 `config/base_config.py` 中：
```python
CRAWLER_MAX_NOTES_COUNT = 100  # 最大爬取数量
```

在 `reddit/client.py` 中：
```python
delay = random.uniform(2.0, 4.0)  # 请求延迟2-4秒
```

---

## 🎯 使用场景

### **场景1：快速分析**
```
目标：快速了解TSLA的最新情绪
操作：输入"TSLA" → 点击生成 → 查看结果
时间：约10-15秒
```

### **场景2：深度分析**
```
目标：分析过去48小时的市场情绪
操作：
  1. 设置时间窗口为48小时
  2. 设置最大帖子数为200条
  3. 点击生成
时间：约20-30秒
```

### **场景3：批量分析**
```
目标：分析多个股票
操作：
  1. 分析TSLA → 等待完成
  2. 分析NVDA → 等待完成
  3. 分析IONQ → 等待完成
建议：每次间隔5-10秒
```

---

## ⚠️ 注意事项

### **1. VPN要求**
- ✅ 使用美国/加拿大/欧洲节点
- ❌ 避免日本节点（容易403）

### **2. 爬取频率**
- 建议：每10分钟爬取1个关键词
- 避免：短时间内大量请求

### **3. 数据量**
- 推荐：100条帖子（平衡数据量和速度）
- 最大：200条帖子（深度分析）
- 最小：50条帖子（快速预览）

### **4. API配额**
- Google Gemini免费层：每分钟15个请求
- 每天1500个请求
- 足够日常使用

---

## 🐛 故障排查

### **问题1：爬取失败（403错误）**
**原因**：VPN节点被封
**解决**：
1. 切换VPN节点（推荐美国）
2. 等待5分钟后重试

### **问题2：摘要生成失败**
**原因**：Google API配置错误
**解决**：
1. 检查 `.env` 文件中的 `GOOGLE_API_KEY`
2. 重启Docker容器

### **问题3：没有数据**
**原因**：关键词太冷门或时间窗口太短
**解决**：
1. 尝试更热门的关键词
2. 增加时间窗口到48-72小时

### **问题4：进度卡住**
**原因**：网络问题或API超时
**解决**：
1. 刷新页面
2. 检查网络连接
3. 查看Docker日志

---

## 📚 相关文档

- **Reddit爬取配置**: `docs/REDDIT_NO_OAUTH_SOLUTION.md`
- **Google Gemini配置**: `docs/DAILY_DIGEST_GEMINI_SETUP.md`
- **代码实现**: `DailyDigest/core.py`
- **界面代码**: `SingleEngineApp/daily_digest_streamlit_app.py`

---

## 🎉 总结

现在你可以：
1. ✅ **一键完成**：爬取 + 分析
2. ✅ **实时查看**：进度和日志
3. ✅ **灵活配置**：数量和时间
4. ✅ **精美展示**：卡片和摘要

**无需再手动在容器内执行命令！** 🚀
